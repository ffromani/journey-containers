Journey to the Center of the Containers
What happens on my box when a container is started?
10 Mar 2018
Tags: containers,kubernetes,CRI,docker

Francesco Romani
Senior Software Engineer, Red Hat
fromani {gmail,redhat}
http://github.com/{mojaves,fromanirh}

* whoami
- sweng @ Red Hat: opinions and mistakes are my own!
- works daily(-ish): oVirt, libvirt, kvm, python, C
- interested in: golang, containers, kubernetes
- happy linux user (red hat linux, debian, ubuntu, fedora)
- geek 
- not-so-great at drawing (just wait a few slides)

* Goal of this Talk

Focus on container runtime: the "how" not the "what"

Understand the steps that brings a container to life

- What a container runtime does do?
- What is a container runtime, actually?

* NON-Goals of this Talk:

We will *not* talk about:

- container images (tags, layers...)
- how to build images: assume pre-built image
- how to store and fetch images: assume image on local file system

* There and Back Again
{hand-drawn map}

* The Passage of the Marshes
- let's follow a trail: the kubernetes CRI api
- kube concepts are de facto more generic/reusable
- docker is a silo
- mapping to docker concepts is simple

* So, what is a container?
What we commonly call a container is
- a set of the features of the linux kernel we use
- to (re)create a controlled process
- to run a well-known image

* And what is a container runtime?
Any tool, or set of tools which can run _container_ _image_

Examples:
- crio
- containerd
- rkt (same notes about docker)

Some well known names:

- *Docker* *is* a container runtime, but it is also much more
- *Kubernetes* is a management platform, requires a container runtime
- *runC* bare-bones container runtime (you most often want to augment it)

* A recipe for containers
The basic building blocks:
- namespaces: process isolation
- cgroups: resource limits

Security enforcement tools:
- seccomp: limit syscall usage
- SELinux: mandatory access control
- linux capabilities: finer-grained privileges

* What about pods?
A pod is a group of containers run in a shared context
(A pod with exactly one container is perfectly fine!)

The minimum schedulable entity for kubernetes

Nicely composable with containers (with what we commonly mean with "containers")

We want to distinguish to understand some key concepts later on.

.link https://kubernetes.io/docs/concepts/workloads/pods/pod/ Would you like to know more?

* The Building Blocks, Revisited: namespaces are the new hot topic

* Namespaces: Intro

Inception: ~2002; major developments ~2006 and onwards.

A namespace...

  wraps a global system resource in an abstraction that makes it appear to the processes
  within the namespace that they have their own isolated instance of the global resource.
  [...]
  One use of namespaces is to implement containers.

Namespaces are _ephemeral_ by default: they are tied to the lifetime of a process.
Once that process is gone, so is the namespace.

But we can improve this (more on later).

.link http://man7.org/linux/man-pages/man7/namespaces.7.html more documentation

* Namespaces: API

A *Kernel* API, syscalls:

- unshare(2): move calling process in new namespace(s) - and more.
- setns(2): make the calling process join existing namespace(s)
- clone(2): create a new process, optionally joining a new namespace - and *much* more.

* Namespaces: what we can unshare?

- cgroup: cgroup root directory (more on that later)
- ipc: System V IPC, POSIX message queues
- network: network devices, stacks, ports, etc.
- mount: mount points
- pid: process id hierarchy
- user: user and group IDs
- uts: hostname and NIS domain name

.link http://man7.org/linux/man-pages/man7/namespaces.7.html more documentation

* Namespaces: /procfs goodies

  Each process has a /proc/$PID/ns/ subdirectory containing one entry
  for each namespace that supports being manipulated by setns(2)

Actually, the setns(2) syscall accepts a _file_ _descriptor_ as parameter.
Everything is a file!

  Bind mounting (see mount(2)) one of the files in this directory to
  somewhere else in the filesystem keeps the corresponding namespace of
  the process specified by pid alive even if all processes currently in
  the namespace terminate.

So we can make one namespace outlive a process

.link http://man7.org/linux/man-pages/man7/namespaces.7.html more documentation

* Namespaces DIY: unshare

PID of the current shell:

  ///samurai7/~># echo $$
  5184

We start a new process (bash) with different network and PID namespaces

  ///samurai7/~># unshare --net --fork --pid --mount-proc bash
  ///samurai7/~># echo $$
  1
  ///samurai7/~># ifconfig
  ///samurai7/~>#

Let's doublecheck:

  ///samurai7/~># ls -lh /proc/{1,5184,5282}/ns/pid
  lrwxrwxrwx. 1 root root 0 Feb 21 19:54 /proc/1/ns/pid -> pid:[4026531836]
  lrwxrwxrwx. 1 root root 0 Feb 21 19:53 /proc/5184/ns/pid -> pid:[4026531836]
  lrwxrwxrwx. 1 root root 0 Feb 21 19:54 /proc/5282/ns/pid -> pid:[4026532544]

* Namespaces DIY: nsenter

Let's enter the namespaces context we created in the slide before:

  ///samurai7/~># nsenter -a -t 5282 /bin/sh
  sh-4.4# ps -fauxw
  USER       PID %CPU %MEM    VSZ   RSS TTY      STAT START   TIME COMMAND
  root        32  0.0  0.0 122680  3864 pts/4    S    20:00   0:00 /bin/sh
  root        33  0.0  0.0 149756  3700 pts/4    R+   20:00   0:00  \_ ps -fauxw
  root         1  0.0  0.0 123884  5108 pts/2    S+   19:53   0:00 bash
  sh-4.4# echo $$
  32

* Namespaces DIY: ip

Let's create a new network namespace `cnt`

  ///samurai7/~># ip netns add cnt
  ///samurai7/~># ip netns list
  cnt
  ///samurai7/~># ls -lh /var/run/netns/cnt
  -r--r--r--. 1 root root 0 Feb 23 19:15 /var/run/netns/cnt

And we connect it to the outside:

  ///samurai7/~># ip link add veth0 type veth peer name veth1
  ///samurai7/~># ip link list | grep veth
  7: veth1@veth0: <BROADCAST,MULTICAST,M-DOWN> mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000
  8: veth0@veth1: <BROADCAST,MULTICAST,M-DOWN> mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000
  ///samurai7/~># ip link set veth1 netns cnt
  ///samurai7/~># ip netns exec cnt ip link list
  1: lo: <LOOPBACK> mtu 65536 qdisc noop state DOWN mode DEFAULT group default qlen 1000
      link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
  7: veth1@if8: <BROADCAST,MULTICAST> mtu 1500 qdisc noop state DOWN mode DEFAULT group default qlen 1000
      link/ether 5a:68:bc:a2:4b:c0 brd ff:ff:ff:ff:ff:ff link-netnsid 0

* Namespaces: wrap up

Namespaces allow us to have separate instances of system resources.

*Operating* *System* resources are still *shared*

With the linux namespaces, we have the bare bones of a simpl{e,istic} container engine!

But much more is needed.

* The Building Blocks, Revisited: for isolation, meet many old friends

* cgroups: intro

Inception: ~2007. Major update: ~2013

Linux *C* ontrol *Groups*: allow process to be organized in hierarical groups to
do limiting and accounting of certain system resources.

Most notably, memory and CPU time (and more: block I/O, pids...)

Powerful and easy-as-possible resource control mechanism

But still quite complex to manage

* cgroups: what can we control?

- blkio: limits on input/output access to and from devices
- cpu: uses the scheduler to provide cgroup tasks access to the CPU
- cpuacct: automatic reports on CPU resources used by tasks
- cpuset: assigns individual CPUs and memory nodes to tasks
- memory: sets limits on memory and reports on memory resources
- perf_event: performance analysis.

Specific Linux Distribution (e.g. RHEL) may offer more cgroups.

Add your own!

* cgroups: API

Just use sysfs:

  echo browser_pid > /sys/fs/cgroup/<restype>/<userclass>/tasks

command line tools:  cgcreate, cgexec, and cgclassify (from libcgroup).

Or just let your management engine do that for you:

- systemd
- libvirt
- docker
- any CRI-compatible runtime (more in the next slides)

* cgroups: DIY

Mostly, you don't want to do it :)

.image https://imgs.xkcd.com/comics/automation.png

Seriously, the management tool (whatever it is) almost always Just Works (tm) and
it is simpler to tune.

* cgroups: wrap-up

CGroups provide resource `limit` and `accounting`

Organized in hierarchies

A *LOT* of subtleties with respect to accounting and sensible limits

Here's why you should not DIY - don't reinvent a square wheel

Deserves a (long) talk on its own

* seccomp

Inception: ~2005; Major update ~2012

Operational modes:

- 0 disabled
- 1 for strict: only _four_ system calls: read, write, exit, sigreturn
- 2 for filter: allow developers to write filters to determine if a given syscall can run

* seccomp: API & DIY

Kernel API (syscall), so just prctl(2) and seccomp(2)

And obviously `procfs` interface.

You can add your own syscall filters using *BPF* language (!!!)

Again, better don't reinvent the wheel, just use profiles from your management engine

.link https://lwn.net/Articles/656307/ If you really want to DIY, maybe start here

* SELinux

Inception: ~1998

Adds Mandatory Access Control (MAC) and Role Based Access Control (RBAC) to the linux kernel

Linux, being UNIX-Like, previously supported only Discretionary Access Control

* SELinux: DAC vs MAC vs RBAC

WARNING: brutal semplification ahead

DAC: access control is based on the discretion of the owner: root can do anything.

MAC: the system (and not the users) specifies which can access what: no, even root _cannot_ do that.

RBAC: in a nutshell, generalization of MAC: create and manage _Roles_ to specify which entity can access which data.

.link https://en.wikipedia.org/wiki/Role-based_access_control beware: Again: the world is much more complex than that...

* SELINUX: Daily usage

Mostly used on CentOS, Fedora, RHEL, RHEL-derived distributions

SELinux used to be perceived as overly complex, and overly annoying too.

"Just disable SELinux" was a recurrent advice up until not so long ago

It got *EXTREMELY* better: most of time, you don't even notice it is running. Just Works (tm)

Except when it prevents exploits :)

If you need to troubleshoot something, `audit2why` is usally a great start

Again, most often just use the profiles your distribution/management engine provides

.link http://selinuxproject.org/page/Main_Page Lots of documentation available

* We have the tools, let's build something

* Kubernetes

  Kubernetes is an open-source system for automating deployment,
  scaling, and management of containerized applications.

.link https://kubernetes.io/ source

Used to require Docker to actually run containers

.link http://blog.kubernetes.io/2016/12/container-runtime-interface-cri-in-kubernetes.html Became runtime-agnostic since version 1.5

* Kubelet

Node-agent of kubernetes: runs on every worker node of your cluster

Makes sure that containers described by POD specifications are running and healthy

See few slides before for the definition of POD and Container

.link https://kubernetes.io/docs/reference/generated/kubelet/ more documentation

* The CRI API

Introduced in Kubernetes 1.5

TODO: redo image

.image https://github.com/containerd/cri-containerd/raw/master/docs/cri-containerd.png _ 800

Took some time to really have valid alternatives

Plugin interface which enables kubelet to use different container runtimes

Previously, to use a different runtime (e.g. not docker), patches to kubelet were needed.

* Noteworthy CRI-compliant runtimes

.link https://github.com/kubernetes-incubator/cri-o cri-o: OCI Kubernetes Container Runtime daemon

.link https://github.com/kubernetes-incubator/rktlet rtklet: rkt-based implementation of Kubernetes CRI

.link https://github.com/containerd/cri-containerd cri-containerd: containerd-based implementation of Kubernetes CRI.

.link https://github.com/kubernetes/frakti frakti: lets Kubernetes run pods and containers directly inside hypervisors via runV

* Let's get this process started (1/2)

.link https://github.com/kubernetes-incubator/cri-tools crictl: CRI command line interface

- command line is concise, easy to paste in the slides :)
- simple enough to not get in the way
- 1:1 mapping to protocol

crictl configuration (/etc/crictl.yaml): specify the UNIX domain socket to use to connect to the CRI runtime

obviously, each runtime uses a different path :(

* Let's get this process started (2/2)

CRI compliant runtime: anything is fine. We pick cri-containerd because has a nice bundle of
crictl, configuration file and runtime

WARNING: be careful to pick the right version of crictl and CRI runtime

* The CRI interface API

.link https://github.com/kubernetes/kubernetes/blob/master/pkg/kubelet/apis/cri/runtime/v1alpha2/api.proto from the protocol spec:

  service RuntimeService {
    // Sandbox operations.
    rpc RunPodSandbox(RunPodSandboxRequest) returns (RunPodSandboxResponse) {}
    rpc PodSandboxStatus(PodSandboxStatusRequest) returns (PodSandboxStatusResponse) {}
    rpc ListPodSandbox(ListPodSandboxRequest) returns (ListPodSandboxResponse) {}
    ...
    // Container operations.
    rpc CreateContainer(CreateContainerRequest) returns (CreateContainerResponse) {}
    rpc StartContainer(StartContainerRequest) returns (StartContainerResponse) {}
    rpc ListContainers(ListContainersRequest) returns (ListContainersResponse) {}
    rpc ContainerStatus(ContainerStatusRequest) returns (ContainerStatusResponse) {}
    ...
  }

* Create workflow (aka: what to expect next)

- define a "sandbox" (in CRI jargon) using a JSON spec
- create and "run" it -> *RuntimeService.RunPodSandBox*
- define containers again using a JSON spec -> *RuntimeService.CreateContainer*
- create container(s) in the sandbox -> *RuntimeService.StartContainer*

* Sandbox bootstrap

create and run

  # cat sandbox-config.json
  {
    "metadata": {
        "name": "test-sandbox",
        "namespace": "default",
        "attempt": 1,
        "uid": "hdishd83djaidwnduwk28bcsb"
    },
    "linux": {
    }
  }
  # crictl create sandbox-config.json
  # crictl runs sandbox-config.json

inspect:

  # crictl sandboxes
  SANDBOX ID          CREATED             STATE               NAME                NAMESPACE           ATTEMPT
  959fcb9d9207a       3 minutes ago       SANDBOX_READY       test-sandbox        default             1

* Sandbox: what's in there?

  USER       PID %CPU %MEM    VSZ   RSS COMMAND
  root      1436  0.0  0.3 854920 27176 /usr/local/bin/containerd
  root      1823  0.0  0.0   8928  4184  \_ containerd-shim -namespace k8s.io ...
  root      1839  0.0  0.0   1028     4      \_ /pause
  root      1446  0.0  0.2 398328 22364 /usr/local/bin/cri-containerd ...

Uhm, pause?

Step back for a sec.

* What is a sandbox, by the way?

* The pause container

* And now something different: the network setup
- CNI

* CNI plugins

* CNI proto

* CNI steps

* Containers in a pod

???

* wrap-up/takeaways

- a container is the execution of a given _image_

- under the hood containers are agglomerates of cooperating features of the Linux Kernel

- many of those features are quite old, and only recently they started to be integrated

- lot of moving parts, *and* the environment is still evolving and changing

- interoperability is improving (also thanks to kubernetes and CRI?)

* Q? A!

